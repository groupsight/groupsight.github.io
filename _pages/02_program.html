---
layout: page
title: Program
permalink: /program/
---

<h2>Keynote Speakers</h2>

<div class="organizers">

    <div class="row profile-row">
        <div class="col-md-6">
            <div class="row">
                <div class="col-sm-4 img-center"><img class="img-responsive photo-size img-rounded" src="https://crowd.cs.vt.edu/groupsight/meredithmorris.jpg"></div>
                <div class="col-sm-8">
                    <h3 class="section-heading"><a class="theme-link-org" href="http://merrie.info">Meredith Ringel Morris</a></h3>
                    <p class="lead">
                        Microsoft Research
                    </p>
                    <p><i>Combining Human and Machine Intelligence to Describe Images to People with Visual Impairments</i></p>
                    <p><a href="/media/Morris_Keynote_GroupSight2017.pptx">Download Slides</a></p>
                    <p>Digital imagery pervades modern life. More than a billion images per day are produced and uploaded to social media sites, and we also encounter digital images within websites, apps, digital documents, and eBooks. Engaging with digital imagery is part of the fabric of participation in contemporary society, including education, the professions, e-commerce, civic participation , entertainment, and social interactions. However, most digital images remain inaccessible to the 39 million people worldwide who are blind. By some estimates, nearly half of online images lack any alternative text descriptions that can be read aloud by screen reader software, and many images that do contain alt text have captions that are of poor quality; many popular emerging formats like social media and mobile apps do not even offer content authors the ability to specify caption information. Emerging AI techniques, such as vision-to-language systems, offer a cheap, scalable means of labeling digital images; however, these technologies have a long way to go before they can be a reliable information source for people who are visually impaired. To help supplement, correct, and train AI captioning systems, human-in-the-loop techniques such as crowdsourcing and friendsourcing can play an important role in advancing caption coverage and quality. In this talk, I will discuss the tradeoffs of various image-description techniques, and present example hybrid intelligence systems for making digital imagery accessible to screen reader users.</p>
                    <p><i>Meredith Ringel Morris is a Principal Researcher at Microsoft Research; she is also an affiliate Professor at the University of Washington in both the School of Computer Science & Engineering and the School of Information. Dr. Morris’s research focuses on human-computer interaction, specializing in computer-supported cooperative work and social computing. Her past research contributions have included interaction techniques to support group work around large, shared displays and novel systems supporting collaborative and social web search. Her current research focuses on the intersection of accessibility and social technologies. Dr. Morris earned her Ph.D. and Master’s degrees in computer science from Stanford University, and her Sc.B. in computer science from Brown University. More information about her research, including her full list of publications, can be found at http://merrie.info.</i></p>
                    <hr style="border-color: #939393; width: 50px" align="left">
                </div>
            </div>
        </div>
        <div class="col-md-6">
            <div class="row">
                <div class="col-sm-4 img-center"><img class="img-responsive photo-size img-rounded" src="https://crowd.cs.vt.edu/groupsight/walterlasecki.jpg"></div>
                <div class="col-sm-8">
                    <h3 class="section-heading"><a class="theme-link-org" href="https://web.eecs.umich.edu/~wlasecki/">Walter Lasecki</a></h3>
                    <p class="lead">
                      University of Michigan
                    </p>
                    <p><i>Real-Time Crowdsourcing for On-Demand Training of Computer Vision Systems</i></p>
                    <p>Systems that see and understand visual scenes can help people with disabilities better access the world around them, help complete dangerous jobs in hazardous conditions, and generally allow us more control over our physical environments. Computer vision has had significant and wide-spread success with machine learning-based approaches for specific classes of problems, but effectively transferring that knowledge to new, more general domains is an open problem. Thus, generating the massive, tailored training data sets that are needed to retrain these ML algorithms for use in new settings is a critical challenge. Crowdsourcing has provided a means of collecting data at scale, but is typically an offline/batch process that takes days or weeks to generate data. Significant prior work has focused on improving efficiency in this context. In this talk, I argue that the future of large-scale computer vision lies in on-demand streams of data that allow training to be done on-the-fly. The resulting systems are more robust, more flexible, and more efficient than those using a priori training data. I will discuss my lab’s work on real-time crowdsourcing and show how human insight can be brought to bear when and where they are encountered (within seconds or less) by intelligent systems in real-world scenarios.</p>
                    <p><i>Walter S. Lasecki is an Assistant Professor of Computer Science and Engineering at the University of Michigan, Ann Arbor, where he directs the Crowds+Machines (CROMA) Lab. He and his students create interactive intelligent systems that are robust enough to be used in real-world settings by combining both human and machine intelligence to exceed the capabilities of either. These systems let people be more productive and help improve access to the world for people with disabilities. Dr. Lasecki received his Ph.D and M.S. from the University of Rochester in 2015 and a B.S. in Computer Science and Mathematics from Virginia Tech in 2010. He has previously held visiting research positions at CMU, Stanford, Microsoft Research, and Google[x].</i></p>
                    <hr style="border-color: #939393; width: 50px" align="left">
                </div>
            </div>
        </div>
     </div>

</div>

<h2>Accepted Papers</h2>

<ul style="list-style: none; margin-left: 0; padding-left: 0;">
  <li style="margin-bottom: 1em;"><strong>IntoFocus: Privacy-preserving Crowd-powered Image Redaction</strong>
    <br>Abdullah Alshaibani, Li-Hsin Tseng, Sylvia Carrell, Alexander J. Quinn (Purdue University)
    <br><em>Best Paper Award</em></li>

  <li style="margin-bottom: 1em;"><a href="/media/Exprgram_GroupSight2017.pdf"><strong>Exprgram: A Video-based Language Learning Interface Powered by Learnersourced Video Annotations</strong></a>
    <br>Kyung Je Jo, John Joon Young Chung, Juho Kim (KAIST)
    <br><em>Best Paper Runner-Up Award</em></li>

  <li style="margin-bottom: 1em;"><a href="/media/GroundTruth_GroupSight2017.pdf"><strong>GroundTruth: Bringing Together Experts and Crowds for Image Geolocation</strong></a>
    <br>Rachel Kohler, John Purviance, Kurt Luther (Virginia Tech)</li>

  <li style="margin-bottom: 1em;"><strong>Let’s Agree to Disagree: A Meta-Analysis of disagreement among crowdworkers during Visual Question Answering</strong>
    <br>Anuparna Banerjee, Samridhi Ojha, Danna Gurari (UT Austin)</li>

  <li style="margin-bottom: 1em;"><a href="/media/ToolDiversity_GroupSight2017.pdf"><strong>Tool Diversity as a Means of Improving Aggregate Crowd Performance on Image Segmentation Tasks</strong></a>
    <br>Jean Y. Song, Raymond Fok, Fan Yang, Kyle Wang, Alan Lundgard, Walter S. Lasecki (University of Michigan)</li>

  <li style="margin-bottom: 1em;"><a href="/media/MulticlassBCI_GroupSight2017.pdf"><strong>Towards Multiclass Brain-Computer Interface for Joint Human- Computer Image Analysis</strong></a>
    <br>Brent J. Lance, Anthony J. Ries, Vernon J. Lawhern, David T. Slayback, Steven M. Gutstein (U.S. Army Research Laboratory)</li>

  <li style="margin-bottom: 1em;"><strong>Towards Real-Time Image Captioning using Crowdsourcing and Computer Vision</strong>
    <br>Divya Ramesh (CloudSight)</li>

</ul>

<h2>Schedule</h2>

<p>Location: Beauport Meeting Room on the 2nd floor of the Hilton Quebec Hotel</p>

<table class="table">
  <tr>
    <td style="width: 100px;">9:00</td>
    <td>Welcome</td>
  </tr>
  <tr>
    <td>9:15</td>
    <td>Keynote - Meredith Ringel Morris</td>
  </tr>
  <tr>
    <td>10:00</td>
    <td>2 paper discussions</td>
  </tr>
  <tr>
    <td>10:30</td>
    <td>Break (with posters)</td>
  </tr>
  <tr>
    <td>11:00</td>
    <td>3 paper discussions</td>
  </tr>
  <tr>
    <td>12:00</td>
    <td>Lunch sponsored by Evolv</td>
  </tr>
  <tr>
    <td>1:15</td>
    <td>Keynote - Walter Lasecki</td>
  </tr>
  <tr>
    <td>2:00</td>
    <td>2 paper discussions</td>
  </tr>
  <tr>
    <td>2:30</td>
    <td>Themed breakout groups</td>
  </tr>
  <tr>
    <td>3:30</td>
    <td>Break (with posters)</td>
  </tr>
  <tr>
    <td>4:00</td>
    <td>Next steps and awards</td>
  </tr>
  <tr>
    <td>5:00</td>
    <td>End</td>
  </tr>
</table>
